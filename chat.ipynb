{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cd30c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /home/prajwal/miniconda3/envs/certai/lib/python3.10/site-packages (2.16.1)\n",
      "Collecting tensorflowjs\n",
      "  Downloading tensorflowjs-4.22.0-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: librosa in /home/prajwal/miniconda3/envs/certai/lib/python3.10/site-packages (0.11.0)\n",
      "Requirement already satisfied: scikit-learn in /home/prajwal/miniconda3/envs/certai/lib/python3.10/site-packages (1.7.2)\n",
      "Requirement already satisfied: pandas in /home/prajwal/miniconda3/envs/certai/lib/python3.10/site-packages (2.3.2)\n",
      "Requirement already satisfied: numpy in /home/prajwal/miniconda3/envs/certai/lib/python3.10/site-packages (1.26.4)\n",
      "Requirement already satisfied: matplotlib in /home/prajwal/miniconda3/envs/certai/lib/python3.10/site-packages (3.10.6)\n",
      "Requirement already satisfied: seaborn in /home/prajwal/miniconda3/envs/certai/lib/python3.10/site-packages (0.13.2)\n",
      "Requirement already satisfied: joblib in /home/prajwal/miniconda3/envs/certai/lib/python3.10/site-packages (1.5.2)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /home/prajwal/miniconda3/envs/certai/lib/python3.10/site-packages (from tensorflow) (2.3.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/prajwal/miniconda3/envs/certai/lib/python3.10/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /home/prajwal/miniconda3/envs/certai/lib/python3.10/site-packages (from tensorflow) (25.9.23)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /home/prajwal/miniconda3/envs/certai/lib/python3.10/site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/prajwal/miniconda3/envs/certai/lib/python3.10/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in /home/prajwal/miniconda3/envs/certai/lib/python3.10/site-packages (from tensorflow) (3.14.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/prajwal/miniconda3/envs/certai/lib/python3.10/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in /home/prajwal/miniconda3/envs/certai/lib/python3.10/site-packages (from tensorflow) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/prajwal/miniconda3/envs/certai/lib/python3.10/site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in /home/prajwal/miniconda3/envs/certai/lib/python3.10/site-packages (from tensorflow) (25.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /home/prajwal/miniconda3/envs/certai/lib/python3.10/site-packages (from tensorflow) (4.25.8)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/prajwal/miniconda3/envs/certai/lib/python3.10/site-packages (from tensorflow) (2.32.5)\n",
      "Requirement already satisfied: setuptools in /home/prajwal/miniconda3/envs/certai/lib/python3.10/site-packages (from tensorflow) (78.1.1)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/prajwal/miniconda3/envs/certai/lib/python3.10/site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/prajwal/miniconda3/envs/certai/lib/python3.10/site-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/prajwal/miniconda3/envs/certai/lib/python3.10/site-packages (from tensorflow) (4.15.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/prajwal/miniconda3/envs/certai/lib/python3.10/site-packages (from tensorflow) (1.17.3)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/prajwal/miniconda3/envs/certai/lib/python3.10/site-packages (from tensorflow) (1.75.0)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in /home/prajwal/miniconda3/envs/certai/lib/python3.10/site-packages (from tensorflow) (2.16.2)\n",
      "Requirement already satisfied: keras>=3.0.0 in /home/prajwal/miniconda3/envs/certai/lib/python3.10/site-packages (from tensorflow) (3.11.3)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/prajwal/miniconda3/envs/certai/lib/python3.10/site-packages (from tensorflow) (0.37.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/prajwal/miniconda3/envs/certai/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/prajwal/miniconda3/envs/certai/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/prajwal/miniconda3/envs/certai/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/prajwal/miniconda3/envs/certai/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (2025.8.3)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/prajwal/miniconda3/envs/certai/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.9)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/prajwal/miniconda3/envs/certai/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/prajwal/miniconda3/envs/certai/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.1.3)\n",
      "Collecting flax>=0.7.2 (from tensorflowjs)\n",
      "  Downloading flax-0.10.7-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting importlib_resources>=5.9.0 (from tensorflowjs)\n",
      "  Downloading importlib_resources-6.5.2-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting jax>=0.4.13 (from tensorflowjs)\n",
      "  Downloading jax-0.6.2-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting jaxlib>=0.4.13 (from tensorflowjs)\n",
      "  Downloading jaxlib-0.6.2-cp310-cp310-manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: tf-keras>=2.13.0 in /home/prajwal/miniconda3/envs/certai/lib/python3.10/site-packages (from tensorflowjs) (2.16.0)\n",
      "Collecting tensorflow-decision-forests>=1.5.0 (from tensorflowjs)\n",
      "  Downloading tensorflow_decision_forests-1.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: tensorflow-hub>=0.16.1 in /home/prajwal/miniconda3/envs/certai/lib/python3.10/site-packages (from tensorflowjs) (0.16.1)\n",
      "Collecting packaging (from tensorflow)\n",
      "  Downloading packaging-23.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: audioread>=2.1.9 in /home/prajwal/miniconda3/envs/certai/lib/python3.10/site-packages (from librosa) (3.0.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in /home/prajwal/miniconda3/envs/certai/lib/python3.10/site-packages (from librosa) (0.62.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/prajwal/miniconda3/envs/certai/lib/python3.10/site-packages (from librosa) (1.15.3)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /home/prajwal/miniconda3/envs/certai/lib/python3.10/site-packages (from librosa) (5.2.1)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in /home/prajwal/miniconda3/envs/certai/lib/python3.10/site-packages (from librosa) (0.13.1)\n",
      "Requirement already satisfied: pooch>=1.1 in /home/prajwal/miniconda3/envs/certai/lib/python3.10/site-packages (from librosa) (1.8.2)\n",
      "Requirement already satisfied: soxr>=0.3.2 in /home/prajwal/miniconda3/envs/certai/lib/python3.10/site-packages (from librosa) (1.0.0)\n",
      "Requirement already satisfied: lazy_loader>=0.1 in /home/prajwal/miniconda3/envs/certai/lib/python3.10/site-packages (from librosa) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in /home/prajwal/miniconda3/envs/certai/lib/python3.10/site-packages (from librosa) (1.1.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/prajwal/miniconda3/envs/certai/lib/python3.10/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/prajwal/miniconda3/envs/certai/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/prajwal/miniconda3/envs/certai/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/prajwal/miniconda3/envs/certai/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/prajwal/miniconda3/envs/certai/lib/python3.10/site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/prajwal/miniconda3/envs/certai/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/prajwal/miniconda3/envs/certai/lib/python3.10/site-packages (from matplotlib) (4.60.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/prajwal/miniconda3/envs/certai/lib/python3.10/site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: pillow>=8 in /home/prajwal/miniconda3/envs/certai/lib/python3.10/site-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/prajwal/miniconda3/envs/certai/lib/python3.10/site-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/prajwal/miniconda3/envs/certai/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Collecting optax (from flax>=0.7.2->tensorflowjs)\n",
      "  Downloading optax-0.2.6-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting orbax-checkpoint (from flax>=0.7.2->tensorflowjs)\n",
      "  Downloading orbax_checkpoint-0.11.25-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting tensorstore (from flax>=0.7.2->tensorflowjs)\n",
      "  Downloading tensorstore-0.1.77-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (21 kB)\n",
      "Requirement already satisfied: rich>=11.1 in /home/prajwal/miniconda3/envs/certai/lib/python3.10/site-packages (from flax>=0.7.2->tensorflowjs) (14.1.0)\n",
      "Requirement already satisfied: PyYAML>=5.4.1 in /home/prajwal/miniconda3/envs/certai/lib/python3.10/site-packages (from flax>=0.7.2->tensorflowjs) (6.0.2)\n",
      "Collecting treescope>=0.1.7 (from flax>=0.7.2->tensorflowjs)\n",
      "  Downloading treescope-0.1.10-py3-none-any.whl.metadata (6.6 kB)\n",
      "INFO: pip is looking at multiple versions of jax to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting jax>=0.4.13 (from tensorflowjs)\n",
      "  Downloading jax-0.6.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting jaxlib>=0.4.13 (from tensorflowjs)\n",
      "  Downloading jaxlib-0.6.1-cp310-cp310-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
      "Collecting jax>=0.4.13 (from tensorflowjs)\n",
      "  Downloading jax-0.6.0-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting jaxlib>=0.4.13 (from tensorflowjs)\n",
      "  Downloading jaxlib-0.6.0-cp310-cp310-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
      "Collecting flax>=0.7.2 (from tensorflowjs)\n",
      "  Downloading flax-0.10.6-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting jax>=0.4.13 (from tensorflowjs)\n",
      "  Downloading jax-0.5.3-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting jaxlib>=0.4.13 (from tensorflowjs)\n",
      "  Downloading jaxlib-0.5.3-cp310-cp310-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
      "Collecting jax>=0.4.13 (from tensorflowjs)\n",
      "  Downloading jax-0.5.2-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting jaxlib>=0.4.13 (from tensorflowjs)\n",
      "  Downloading jaxlib-0.5.1-cp310-cp310-manylinux2014_x86_64.whl.metadata (978 bytes)\n",
      "INFO: pip is still looking at multiple versions of jax to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting jax>=0.4.13 (from tensorflowjs)\n",
      "  Downloading jax-0.5.1-py3-none-any.whl.metadata (22 kB)\n",
      "  Downloading jax-0.5.0-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting jaxlib>=0.4.13 (from tensorflowjs)\n",
      "  Downloading jaxlib-0.5.0-cp310-cp310-manylinux2014_x86_64.whl.metadata (978 bytes)\n",
      "Collecting jax>=0.4.13 (from tensorflowjs)\n",
      "  Downloading jax-0.4.38-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting jaxlib>=0.4.13 (from tensorflowjs)\n",
      "  Downloading jaxlib-0.4.38-cp310-cp310-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
      "Collecting jax>=0.4.13 (from tensorflowjs)\n",
      "  Downloading jax-0.4.37-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting jaxlib>=0.4.13 (from tensorflowjs)\n",
      "  Downloading jaxlib-0.4.36-cp310-cp310-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
      "Collecting jax>=0.4.13 (from tensorflowjs)\n",
      "  Downloading jax-0.4.36-py3-none-any.whl.metadata (22 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Downloading jax-0.4.35-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting jaxlib>=0.4.13 (from tensorflowjs)\n",
      "  Downloading jaxlib-0.4.35-cp310-cp310-manylinux2014_x86_64.whl.metadata (983 bytes)\n",
      "Collecting jax>=0.4.13 (from tensorflowjs)\n",
      "  Downloading jax-0.4.34-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting jaxlib>=0.4.13 (from tensorflowjs)\n",
      "  Downloading jaxlib-0.4.34-cp310-cp310-manylinux2014_x86_64.whl.metadata (983 bytes)\n",
      "Collecting flax>=0.7.2 (from tensorflowjs)\n",
      "  Downloading flax-0.10.5-py3-none-any.whl.metadata (11 kB)\n",
      "  Downloading flax-0.10.4-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: namex in /home/prajwal/miniconda3/envs/certai/lib/python3.10/site-packages (from keras>=3.0.0->tensorflow) (0.1.0)\n",
      "Requirement already satisfied: optree in /home/prajwal/miniconda3/envs/certai/lib/python3.10/site-packages (from keras>=3.0.0->tensorflow) (0.17.0)\n",
      "Requirement already satisfied: llvmlite<0.46,>=0.45.0dev0 in /home/prajwal/miniconda3/envs/certai/lib/python3.10/site-packages (from numba>=0.51.0->librosa) (0.45.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /home/prajwal/miniconda3/envs/certai/lib/python3.10/site-packages (from pooch>=1.1->librosa) (4.4.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/prajwal/miniconda3/envs/certai/lib/python3.10/site-packages (from rich>=11.1->flax>=0.7.2->tensorflowjs) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/prajwal/miniconda3/envs/certai/lib/python3.10/site-packages (from rich>=11.1->flax>=0.7.2->tensorflowjs) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/prajwal/miniconda3/envs/certai/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=11.1->flax>=0.7.2->tensorflowjs) (0.1.2)\n",
      "Requirement already satisfied: cffi>=1.0 in /home/prajwal/miniconda3/envs/certai/lib/python3.10/site-packages (from soundfile>=0.12.1->librosa) (2.0.0)\n",
      "Requirement already satisfied: pycparser in /home/prajwal/miniconda3/envs/certai/lib/python3.10/site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.23)\n",
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting wurlitzer (from tensorflow-decision-forests>=1.5.0->tensorflowjs)\n",
      "  Downloading wurlitzer-3.1.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting tf-keras>=2.13.0 (from tensorflowjs)\n",
      "  Using cached tf_keras-2.20.1-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting ydf>=0.11.0 (from tensorflow-decision-forests>=1.5.0->tensorflowjs)\n",
      "  Downloading ydf-0.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
      "Collecting tensorboard~=2.19.0 (from tensorflow)\n",
      "  Downloading tensorboard-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting ml-dtypes>=0.2.0 (from jax>=0.4.13->tensorflowjs)\n",
      "  Using cached ml_dtypes-0.5.3-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (8.9 kB)\n",
      "INFO: pip is looking at multiple versions of tf-keras to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting tf-keras>=2.13.0 (from tensorflowjs)\n",
      "  Using cached tf_keras-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/prajwal/miniconda3/envs/certai/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow) (3.0.2)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 (from tensorflow)\n",
      "  Downloading protobuf-5.29.5-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
      "Collecting chex>=0.1.87 (from optax->flax>=0.7.2->tensorflowjs)\n",
      "  Downloading chex-0.1.90-py3-none-any.whl.metadata (18 kB)\n",
      "INFO: pip is looking at multiple versions of optax to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting optax (from flax>=0.7.2->tensorflowjs)\n",
      "  Downloading optax-0.2.5-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting toolz>=0.9.0 (from chex>=0.1.87->optax->flax>=0.7.2->tensorflowjs)\n",
      "  Downloading toolz-1.0.0-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting etils[epath,epy] (from orbax-checkpoint->flax>=0.7.2->tensorflowjs)\n",
      "  Downloading etils-1.13.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: nest_asyncio in /home/prajwal/miniconda3/envs/certai/lib/python3.10/site-packages (from orbax-checkpoint->flax>=0.7.2->tensorflowjs) (1.6.0)\n",
      "Collecting aiofiles (from orbax-checkpoint->flax>=0.7.2->tensorflowjs)\n",
      "  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting humanize (from orbax-checkpoint->flax>=0.7.2->tensorflowjs)\n",
      "  Downloading humanize-4.13.0-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting simplejson>=3.16.0 (from orbax-checkpoint->flax>=0.7.2->tensorflowjs)\n",
      "  Downloading simplejson-3.20.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: fsspec in /home/prajwal/miniconda3/envs/certai/lib/python3.10/site-packages (from etils[epath,epy]->orbax-checkpoint->flax>=0.7.2->tensorflowjs) (2025.9.0)\n",
      "Collecting zipp (from etils[epath,epy]->orbax-checkpoint->flax>=0.7.2->tensorflowjs)\n",
      "  Downloading zipp-3.23.0-py3-none-any.whl.metadata (3.6 kB)\n",
      "Downloading tensorflowjs-4.22.0-py3-none-any.whl (89 kB)\n",
      "Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
      "Downloading flax-0.10.4-py3-none-any.whl (451 kB)\n",
      "Downloading importlib_resources-6.5.2-py3-none-any.whl (37 kB)\n",
      "Downloading tensorflow_decision_forests-1.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.1 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m16.1/16.1 MB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m  \u001b[33m0:00:15\u001b[0m eta \u001b[36m0:00:01\u001b[0m0m\n",
      "\u001b[?25hDownloading tensorflow-2.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (644.8 MB)\n",
      "\u001b[2K   \u001b[91m‚îÅ‚îÅ\u001b[0m\u001b[90m‚ï∫\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m37.5/644.8 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:07:12\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Install core ML libraries\n",
    "!pip install tensorflow tensorflowjs librosa scikit-learn pandas numpy matplotlib seaborn joblib\n",
    "\n",
    "# Install Kaggle API for dataset download\n",
    "!pip install kaggle\n",
    "\n",
    "# For audio processing\n",
    "!pip install soundfile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1e70be",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'wake_word_data/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 124\u001b[0m\n\u001b[1;32m    116\u001b[0m detector \u001b[38;5;241m=\u001b[39m WakeWordDetector()\n\u001b[1;32m    118\u001b[0m \u001b[38;5;66;03m# Train model (assuming you have data organized in folders)\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;66;03m# data_dir structure:\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;66;03m# wake_word_data/\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;66;03m#   ‚îú‚îÄ‚îÄ hey_jaan/  (500-1000 wav files of \"hey jaan\")\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m#   ‚îî‚îÄ‚îÄ not_wake_word/  (2000+ wav files of other words/noise)\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m model, history \u001b[38;5;241m=\u001b[39m \u001b[43mdetector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwake_word_data/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 78\u001b[0m, in \u001b[0;36mWakeWordDetector.train_model\u001b[0;34m(self, data_dir, model_save_path)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Train the wake word detection model\"\"\"\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m# Prepare dataset\u001b[39;00m\n\u001b[0;32m---> 78\u001b[0m features, labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# Encode labels\u001b[39;00m\n\u001b[1;32m     81\u001b[0m label_encoder \u001b[38;5;241m=\u001b[39m LabelEncoder()\n",
      "Cell \u001b[0;32mIn[3], line 45\u001b[0m, in \u001b[0;36mWakeWordDetector.prepare_dataset\u001b[0;34m(self, data_dir)\u001b[0m\n\u001b[1;32m     42\u001b[0m labels \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Expecting structure: data_dir/hey_jaan/*.wav and data_dir/not_wake_word/*.wav\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m label_dir \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     46\u001b[0m     label_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(data_dir, label_dir)\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(label_path):\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'wake_word_data/'"
     ]
    }
   ],
   "source": [
    "# wake_word_trainer.py - Complete Wake Word Detection Training Script\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers, models\n",
    "import joblib\n",
    "\n",
    "class WakeWordDetector:\n",
    "    def __init__(self, sample_rate=16000, duration=1.0, n_mfcc=40):\n",
    "        self.sample_rate = sample_rate\n",
    "        self.duration = duration\n",
    "        self.n_mfcc = n_mfcc\n",
    "        self.max_len = int(sample_rate * duration)\n",
    "        \n",
    "    def download_dataset(self, data_dir=\"speech_commands_data\"):\n",
    "        \"\"\"Downloads Google Speech Commands V2 dataset from Kaggle\"\"\"\n",
    "        try:\n",
    "            os.system(f\"kaggle datasets download -d yashdogra/speech-commands -p {data_dir} --unzip\")\n",
    "            print(f\"‚úÖ Dataset downloaded to {data_dir}\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error downloading: {e}\")\n",
    "            print(\"Please manually download from: https://www.kaggle.com/datasets/yashdogra/speech-commands\")\n",
    "            return False\n",
    "    \n",
    "    def extract_features(self, audio_path):\n",
    "        \"\"\"Extract MFCC features from audio file\"\"\"\n",
    "        try:\n",
    "            # Load audio\n",
    "            audio, sr = librosa.load(audio_path, sr=self.sample_rate, duration=self.duration)\n",
    "            \n",
    "            # Pad or truncate to fixed length\n",
    "            if len(audio) > self.max_len:\n",
    "                audio = audio[:self.max_len]\n",
    "            else:\n",
    "                audio = np.pad(audio, (0, self.max_len - len(audio)), 'constant')\n",
    "            \n",
    "            # Extract MFCC features\n",
    "            mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=self.n_mfcc, n_fft=2048, hop_length=512)\n",
    "            mfccs_processed = np.mean(mfccs.T, axis=0)\n",
    "            \n",
    "            return mfccs_processed\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {audio_path}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def create_dataset(self, data_dir, wake_words=['yes', 'no']):\n",
    "        \"\"\"Create wake word dataset from Speech Commands\"\"\"\n",
    "        features = []\n",
    "        labels = []\n",
    "        \n",
    "        print(f\"üîÑ Processing audio files from {data_dir}...\")\n",
    "        \n",
    "        for word_dir in os.listdir(data_dir):\n",
    "            word_path = os.path.join(data_dir, word_dir)\n",
    "            if os.path.isdir(word_path) and word_dir != '_background_noise_':\n",
    "                word = word_dir.lower()\n",
    "                \n",
    "                # Label as wake word or not\n",
    "                if word in [w.lower() for w in wake_words]:\n",
    "                    label = 'wake_word'\n",
    "                    max_samples = None  # Use all wake word samples\n",
    "                else:\n",
    "                    label = 'not_wake_word'\n",
    "                    max_samples = 800  # Limit negative samples to balance dataset\n",
    "                \n",
    "                print(f\"  Processing '{word}' -> {label}\")\n",
    "                \n",
    "                audio_files = [f for f in os.listdir(word_path) if f.endswith('.wav')]\n",
    "                if max_samples:\n",
    "                    audio_files = audio_files[:max_samples]\n",
    "                \n",
    "                for i, audio_file in enumerate(audio_files):\n",
    "                    audio_path = os.path.join(word_path, audio_file)\n",
    "                    feature = self.extract_features(audio_path)\n",
    "                    \n",
    "                    if feature is not None:\n",
    "                        features.append(feature)\n",
    "                        labels.append(label)\n",
    "                    \n",
    "                    if i % 100 == 0 and i > 0:\n",
    "                        print(f\"    Processed {i}/{len(audio_files)} files...\")\n",
    "        \n",
    "        print(f\"‚úÖ Total samples processed: {len(features)}\")\n",
    "        return np.array(features), np.array(labels)\n",
    "    \n",
    "    def create_model(self, input_shape):\n",
    "        \"\"\"Create neural network model\"\"\"\n",
    "        model = tf.keras.Sequential([\n",
    "            layers.Dense(128, activation='relu', input_shape=input_shape),\n",
    "            layers.Dropout(0.3),\n",
    "            layers.BatchNormalization(),\n",
    "            \n",
    "            layers.Dense(256, activation='relu'),\n",
    "            layers.Dropout(0.4),\n",
    "            layers.BatchNormalization(),\n",
    "            \n",
    "            layers.Dense(128, activation='relu'),\n",
    "            layers.Dropout(0.3),\n",
    "            \n",
    "            layers.Dense(64, activation='relu'),\n",
    "            layers.Dropout(0.2),\n",
    "            \n",
    "            layers.Dense(2, activation='softmax')  # Binary classification\n",
    "        ])\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy', 'precision', 'recall']\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def train_model(self, data_dir=\"speech_commands_data\", wake_words=['yes', 'no'], model_save_path=\"hey_jaan_model\"):\n",
    "        \"\"\"Train the complete model\"\"\"\n",
    "        \n",
    "        # Step 1: Download dataset\n",
    "        if not os.path.exists(data_dir):\n",
    "            print(\"üì• Dataset not found. Downloading...\")\n",
    "            if not self.download_dataset(data_dir):\n",
    "                return None, None\n",
    "        \n",
    "        # Step 2: Create dataset\n",
    "        features, labels = self.create_dataset(data_dir, wake_words)\n",
    "        \n",
    "        if len(features) == 0:\n",
    "            print(\"‚ùå No features extracted!\")\n",
    "            return None, None\n",
    "        \n",
    "        # Step 3: Encode labels\n",
    "        label_encoder = LabelEncoder()\n",
    "        y_encoded = label_encoder.fit_transform(labels)\n",
    "        y_categorical = tf.keras.utils.to_categorical(y_encoded)\n",
    "        \n",
    "        # Show label distribution\n",
    "        unique, counts = np.unique(labels, return_counts=True)\n",
    "        print(f\"\\nüìä Label Distribution:\")\n",
    "        for label, count in zip(unique, counts):\n",
    "            print(f\"   {label}: {count} samples\")\n",
    "        \n",
    "        # Step 4: Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            features, y_categorical, test_size=0.2, random_state=42, stratify=labels\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nüìà Data Split:\")\n",
    "        print(f\"   Training: {len(X_train)} samples\")\n",
    "        print(f\"   Testing: {len(X_test)} samples\")\n",
    "        \n",
    "        # Step 5: Create and train model\n",
    "        model = self.create_model((features.shape[1],))\n",
    "        \n",
    "        print(f\"\\nüß† Model Architecture:\")\n",
    "        model.summary()\n",
    "        \n",
    "        # Callbacks for better training\n",
    "        callbacks = [\n",
    "            tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True, monitor='val_accuracy'),\n",
    "            tf.keras.callbacks.ReduceLROnPlateau(patience=5, factor=0.5, monitor='val_loss'),\n",
    "            tf.keras.callbacks.ModelCheckpoint(f\"{model_save_path}_best.h5\", save_best_only=True, monitor='val_accuracy')\n",
    "        ]\n",
    "        \n",
    "        print(f\"\\nüöÄ Starting training...\")\n",
    "        history = model.fit(\n",
    "            X_train, y_train,\n",
    "            epochs=50,\n",
    "            batch_size=32,\n",
    "            validation_data=(X_test, y_test),\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # Step 6: Evaluate\n",
    "        test_loss, test_acc, test_precision, test_recall = model.evaluate(X_test, y_test, verbose=0)\n",
    "        f1_score = 2 * (test_precision * test_recall) / (test_precision + test_recall + 1e-8)\n",
    "        \n",
    "        print(f\"\\nüéØ Final Results:\")\n",
    "        print(f\"   Accuracy: {test_acc:.4f}\")\n",
    "        print(f\"   Precision: {test_precision:.4f}\")\n",
    "        print(f\"   Recall: {test_recall:.4f}\")\n",
    "        print(f\"   F1-Score: {f1_score:.4f}\")\n",
    "        \n",
    "        # Step 7: Save everything\n",
    "        model.save(model_save_path)\n",
    "        joblib.dump(label_encoder, f\"{model_save_path}_label_encoder.pkl\")\n",
    "        \n",
    "        # Save model info\n",
    "        model_info = {\n",
    "            'wake_words': wake_words,\n",
    "            'sample_rate': self.sample_rate,\n",
    "            'duration': self.duration,\n",
    "            'n_mfcc': self.n_mfcc,\n",
    "            'test_accuracy': float(test_acc),\n",
    "            'test_precision': float(test_precision),\n",
    "            'test_recall': float(test_recall),\n",
    "            'f1_score': float(f1_score)\n",
    "        }\n",
    "        joblib.dump(model_info, f\"{model_save_path}_info.pkl\")\n",
    "        \n",
    "        print(f\"\\nüíæ Model saved:\")\n",
    "        print(f\"   Model: {model_save_path}\")\n",
    "        print(f\"   Label encoder: {model_save_path}_label_encoder.pkl\")\n",
    "        print(f\"   Info: {model_save_path}_info.pkl\")\n",
    "        \n",
    "        return model, history\n",
    "    \n",
    "    def convert_to_tfjs(self, model_path, output_path):\n",
    "        \"\"\"Convert to TensorFlow.js\"\"\"\n",
    "        try:\n",
    "            import tensorflowjs as tfjs\n",
    "            model = tf.keras.models.load_model(model_path)\n",
    "            tfjs.converters.save_keras_model(model, output_path)\n",
    "            print(f\"‚úÖ TensorFlow.js model saved to: {output_path}\")\n",
    "            return True\n",
    "        except ImportError:\n",
    "            print(\"‚ùå Install tensorflowjs: pip install tensorflowjs\")\n",
    "            return False\n",
    "\n",
    "# Main training script\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üé§ Wake Word Detection Model Trainer\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Initialize\n",
    "    detector = WakeWordDetector(sample_rate=16000, duration=1.0, n_mfcc=40)\n",
    "    \n",
    "    # Configuration\n",
    "    DATA_DIR = \"speech_commands_data\"\n",
    "    WAKE_WORDS = ['yes', 'no']  # Change to any words from dataset\n",
    "    MODEL_NAME = \"hey_jaan_wake_word_model\"\n",
    "    \n",
    "    print(f\"‚öôÔ∏è  Configuration:\")\n",
    "    print(f\"   Wake words: {WAKE_WORDS}\")\n",
    "    print(f\"   Data directory: {DATA_DIR}\")\n",
    "    print(f\"   Model name: {MODEL_NAME}\")\n",
    "    \n",
    "    # Train model\n",
    "    model, history = detector.train_model(\n",
    "        data_dir=DATA_DIR,\n",
    "        wake_words=WAKE_WORDS,\n",
    "        model_save_path=MODEL_NAME\n",
    "    )\n",
    "    \n",
    "    if model:\n",
    "        print(f\"\\nüéâ Training completed!\")\n",
    "        \n",
    "        # Convert to TensorFlow.js\n",
    "        tfjs_path = f\"{MODEL_NAME}_tfjs\"\n",
    "        detector.convert_to_tfjs(MODEL_NAME, tfjs_path)\n",
    "        \n",
    "        print(f\"\\nüöÄ Ready to use:\")\n",
    "        print(f\"   Python model: {MODEL_NAME}\")\n",
    "        print(f\"   Web model: {tfjs_path}\")\n",
    "        print(f\"\\nüìù Available words in dataset:\")\n",
    "        print(\"   yes, no, up, down, left, right, on, off, stop, go,\")\n",
    "        print(\"   zero, one, two, three, four, five, six, seven, eight, nine,\")\n",
    "        print(\"   bed, bird, cat, dog, happy, house, marvin, sheila, tree, wow\")\n",
    "    else:\n",
    "        print(\"‚ùå Training failed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a95499d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/prajwal/vscode/neonexus-web\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed521e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "certai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
